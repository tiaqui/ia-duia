{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRtwYvGUCelK"
   },
   "source": [
    "# Spam en mensajes de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r62PgfzGZmm8"
   },
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "GUh8tNLf8lph",
    "outputId": "c5b5d77a-c8f0-48cf-f0fc-233d7eec0001"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2   x1   x2   x3\n",
       "0   ham  Go until jurong point, crazy.. Available only ...  NaN  NaN  NaN\n",
       "1   ham                      Ok lar... Joking wif u oni...  NaN  NaN  NaN\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...  NaN  NaN  NaN\n",
       "3   ham  U dun say so early hor... U c already then say...  NaN  NaN  NaN\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...  NaN  NaN  NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Importamos y convertimos los datos a un DataFrame\n",
    "#df_spam = pd.read_csv('io.StringIO(uploaded['spam.csv'].decode('iso8859'))) #, sep='\\t''\n",
    "df_spam = pd.read_csv('datasets/spam.csv', encoding='iso8859')\n",
    "df_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7S0xOVSmYR2_",
    "outputId": "bcb87955-dd02-4cb1-81ae-07973264ba2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorías del dataset\n",
    "df_spam['v1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "41NY-O9JEL3o",
    "outputId": "e3ffd5d0-347e-484e-9d21-168e65f658b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "fhu-6vBqYzuR",
    "outputId": "c0bd13c3-5405-4928-a492-7f05923056d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam['v1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DATB92y-ZBku"
   },
   "source": [
    "Como vemos, este es un dataframe binario, dado que tiene dos categorías únicas. Tiene 5572 filas (ejemplos). Cabe observar que está relativamente desbalanceado, registrándose muchos más elementos en la categoría *ham*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QC8YUNy9I5Yl"
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtblPIU7JBVk"
   },
   "source": [
    "### Tratamiento de valores `NaN`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYJWRltnEpwc"
   },
   "source": [
    "Como podemos ver, para todas las primeras filas, las últimas tres columnas están rellenas con `NaN`. Para determinar si conservar o no las mismas, veamos cuántos y cuáles valores muestran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "QLLbSLiUD0kH",
    "outputId": "1c6a3abe-ae73-43c4-9ed6-d18c6b804ecb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1    5572\n",
       "v2    5572\n",
       "x1      50\n",
       "x2      12\n",
       "x3       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "rIaVS0l_F4yx",
    "outputId": "71d57137-41ee-4d08-faa5-43e055739121"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>PO Box 5249</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\Wen u miss someone</td>\n",
       "      <td>the person is definitely special for u..... B...</td>\n",
       "      <td>why to miss them</td>\n",
       "      <td>just Keep-in-touch\\\" gdeve..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>PO Box 5249</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>ham</td>\n",
       "      <td>Edison has rightly said, \\A fool can ask more ...</td>\n",
       "      <td>GN</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\CAN I PLEASE COME UP NOW IMIN TOWN.DONTMATTER...</td>\n",
       "      <td>JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIG...</td>\n",
       "      <td>U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\"\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1  ...                              x3\n",
       "95    spam  ...                             NaN\n",
       "281    ham  ...   just Keep-in-touch\\\" gdeve..\"\n",
       "899   spam  ...                             NaN\n",
       "1038   ham  ...                         GNT:-)\"\n",
       "2170   ham  ...                             NaN\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam[pd.notna(df_spam.loc[:, 'x2'])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQzYgK1ZJXom"
   },
   "source": [
    "Las columnas restantes aparentan ser la continuación del texto de la columna `v2`, por esto, se decide combinar la información en esta última columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "CwkW1X0aGpvb",
    "outputId": "128b0e33-633e-4dd7-c5e3-f9e9a23a0ae2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Wen u miss someone the person is definitely special for u..... But if the person is so special why to miss them just Keep-in-touch\\\\\" gdeve..\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam.loc[:, 'v2'] = df_spam.loc[:, 'v2' : 'x3'].fillna('').sum(axis=1)\n",
    "df_spam.drop(['x1', 'x2', 'x3'], axis=1, inplace=True)\n",
    "\n",
    "df_spam.loc[281, 'v2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mz7bTzX_OTep"
   },
   "source": [
    "De esta manera, se eliminaron todos los valores `NaN` sin perder los datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0gUX-c5Oalt"
   },
   "source": [
    "### Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsySkYN0aKJq"
   },
   "source": [
    "A continuación, se determina aplicar un preprocesamiento sobre el texto en sí, realizando:\n",
    "\n",
    "*   Remoción de stop words.\n",
    "*   Stemming.\n",
    "*   Inclusión de n-grams (hasta n=2).\n",
    "*   Control de frecuencia mínima (dos documentos).\n",
    "*   Conservación únicamente de tokens alfabéticos.\n",
    "*   Generación de matriz TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQ2LW0DzbtEb"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# Creamos el stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Construimos un tokenizer\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "\n",
    "# Obtenemos las stopwords que provee scikit-learn\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# Definimos una función que aplica el stemming luego de tokenizar y remover stopwords\n",
    "def  stem_tokenizer(doc):\n",
    "    # Aplica la tokenización\n",
    "    tokens = tokenizer(doc)\n",
    "    # Retorna la lista de tokens luego de aplicar stemming si no es un stopword\n",
    "    return list(stemmer.stem(w) for w in tokens if w not in stop_words and w.isalpha()) # definir list(k for k in w if k.isdigit())\n",
    "\n",
    "# Aplicamos el procesamiento\n",
    "vectorizer = TfidfVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2), min_df=2)\n",
    "tokens = vectorizer.fit_transform(df_spam['v2'].tolist())\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df_tokens = pd.DataFrame(tokens.toarray(), columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "4yBOeDqoxulJ",
    "outputId": "0f6f1d8e-3eb2-4262-9a6a-a35be5880d46"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aah</th>\n",
       "      <th>aathi</th>\n",
       "      <th>aathi dear</th>\n",
       "      <th>aathi love</th>\n",
       "      <th>abi</th>\n",
       "      <th>abil</th>\n",
       "      <th>abiola</th>\n",
       "      <th>abj</th>\n",
       "      <th>abl</th>\n",
       "      <th>abl come</th>\n",
       "      <th>abl deliv</th>\n",
       "      <th>abl pay</th>\n",
       "      <th>absolutli</th>\n",
       "      <th>absolutli fine</th>\n",
       "      <th>abt</th>\n",
       "      <th>abt tht</th>\n",
       "      <th>abt ur</th>\n",
       "      <th>abta</th>\n",
       "      <th>abta complimentari</th>\n",
       "      <th>aburo</th>\n",
       "      <th>aburo enjoy</th>\n",
       "      <th>abus</th>\n",
       "      <th>ac</th>\n",
       "      <th>ac stop</th>\n",
       "      <th>acc</th>\n",
       "      <th>accept</th>\n",
       "      <th>accept brother</th>\n",
       "      <th>accept day</th>\n",
       "      <th>access</th>\n",
       "      <th>access number</th>\n",
       "      <th>accid</th>\n",
       "      <th>accid claim</th>\n",
       "      <th>accident</th>\n",
       "      <th>accident delet</th>\n",
       "      <th>accomod</th>\n",
       "      <th>accordingli</th>\n",
       "      <th>account</th>\n",
       "      <th>account balanc</th>\n",
       "      <th>account bank</th>\n",
       "      <th>account detail</th>\n",
       "      <th>...</th>\n",
       "      <th>yiju meet</th>\n",
       "      <th>ym</th>\n",
       "      <th>yo</th>\n",
       "      <th>yo come</th>\n",
       "      <th>yo valentin</th>\n",
       "      <th>yo yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yogasana</th>\n",
       "      <th>yogasana oso</th>\n",
       "      <th>yor</th>\n",
       "      <th>your</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr prize</th>\n",
       "      <th>yummi</th>\n",
       "      <th>yun</th>\n",
       "      <th>yun ah</th>\n",
       "      <th>yunni</th>\n",
       "      <th>yuo</th>\n",
       "      <th>yuo exmpel</th>\n",
       "      <th>yuo ra</th>\n",
       "      <th>yup</th>\n",
       "      <th>yup have</th>\n",
       "      <th>yup lor</th>\n",
       "      <th>yup ok</th>\n",
       "      <th>yup thk</th>\n",
       "      <th>zed</th>\n",
       "      <th>zed logo</th>\n",
       "      <th>zed profit</th>\n",
       "      <th>zoe</th>\n",
       "      <th>åð</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ìï come</th>\n",
       "      <th>ìï dun</th>\n",
       "      <th>ìï got</th>\n",
       "      <th>ìï home</th>\n",
       "      <th>ìï ma</th>\n",
       "      <th>ìï sch</th>\n",
       "      <th>ìï wait</th>\n",
       "      <th>ìï wan</th>\n",
       "      <th>ûò</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7829 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aah  aathi  aathi dear  aathi love  abi  ...  ìï ma  ìï sch  ìï wait  ìï wan   ûò\n",
       "0  0.0    0.0         0.0         0.0  0.0  ...    0.0     0.0      0.0     0.0  0.0\n",
       "1  0.0    0.0         0.0         0.0  0.0  ...    0.0     0.0      0.0     0.0  0.0\n",
       "2  0.0    0.0         0.0         0.0  0.0  ...    0.0     0.0      0.0     0.0  0.0\n",
       "3  0.0    0.0         0.0         0.0  0.0  ...    0.0     0.0      0.0     0.0  0.0\n",
       "4  0.0    0.0         0.0         0.0  0.0  ...    0.0     0.0      0.0     0.0  0.0\n",
       "\n",
       "[5 rows x 7829 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qP5COl6QGZlx"
   },
   "source": [
    "Teniendo un método mediante el cual obtener la matriz resultante, se procede a generar el modelo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7wwjBeIGg7H"
   },
   "source": [
    "## Generación de los modelos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHi6wfFZIBzo"
   },
   "source": [
    "Elegimos en este caso dos modelos a realizar, SVM y Naive Bayes. El primero de estos es un clasificador binario que funciona bien para espacios altamente dimensionales (como lo es el texto), mientras que el segundo suele ser bueno para texto, es robusto a valores faltantes y outliers, y nos ofrece la ventaja de ser incremental.\n",
    "\n",
    "A modo de generalizar el procesamiento aplicado en la celda anterior, se realiza un pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRKWW0kbHlTS"
   },
   "outputs": [],
   "source": [
    "# Obtengo los atributos y las clases por separado\n",
    "X, y = df_spam['v2'].values, df_spam['v1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-biHxOWHqHc"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Genero dos pipelines, con los dos modelos elegidos\n",
    "pipe_a = make_pipeline(\n",
    "     TfidfVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2), min_df=2),\n",
    "     SVC())\n",
    "\n",
    "pipe_b = make_pipeline(\n",
    "     TfidfVectorizer(tokenizer=stem_tokenizer, ngram_range=(1,2), min_df=2),\n",
    "     MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7YNzzaVUpJX"
   },
   "source": [
    "## Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bwB4P4WEui8"
   },
   "source": [
    "Respecto de las métricas a considerar, se destaca el F-1 Score, que combina precisión (predicciones correctas del total de positivas) y recall (del total de ejemplos positivos cuántos fueron correctamente clasificados) en una sola métrica, facilitando la comparación de modelos. Se realiza un promediado con macro-averaging, que aplica igual peso a ambas clases. Se computa además el Accuracy de los modelos y se obtienen métricas respecto el tiempo de entrenamiento y predicción.\n",
    "\n",
    "Se utiliza una variante de k-fold cross validation, que da la ventaja de que todos los ejemplos se utilizan, en algún momento, para entrenamiento y prueba. La variante utilizada es `StratifiedKFold`, dado que intenta preservar la cantidad de muestras por clase. Esto se elige dada la disparidad en los ejemplos para cada clase (siendo la mayoría de la clase `ham`).\n",
    "\n",
    "Se determina utilizar un `k = 10` dado que se tiene una buena cantidad de ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GZ24oBHDzyN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "\n",
    "scoring = ['f1_macro', 'accuracy']\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores_a = cross_validate(pipe_a, X, y, cv=cv, scoring=scoring)\n",
    "scores_b = cross_validate(pipe_b, X, y, cv=cv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "etPZ4G7lg93q",
    "outputId": "c52d6d76-1989-42f6-9492-37cd59142d6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 macro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1 macro  Accuracy  Fit time  Score time\n",
       "SVM              0.96      0.98      2.45        0.23\n",
       "Naive Bayes      0.92      0.97      0.87        0.09"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[scores_a['test_f1_macro'].mean(), scores_a['test_accuracy'].mean(), \n",
    "               scores_a['fit_time'].mean(), scores_a['score_time'].mean()],\n",
    "              [scores_b['test_f1_macro'].mean(), scores_b['test_accuracy'].mean(),\n",
    "               scores_b['fit_time'].mean(), scores_b['score_time'].mean()]]\n",
    "             , index=['SVM', 'Naive Bayes'], \n",
    "             columns=['F1 macro','Accuracy','Fit time','Score time']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9KJdUzr-imOD"
   },
   "source": [
    "Puede verse que se obtienen mejores valores tanto para el F-1 Score como para Accuracy en el clasificador SVM. Por otro lado, el modelo Naive Bayes demuestra ser más rápido tanto en la etapa de entrenamiento como en la predicción.\n",
    "\n",
    "Para finalizar, realizamos predicciones a través de `cross_val_predict()`, y luego mostramos la matriz de confusión para cada uno de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "0Ckk7MPZUH7y",
    "outputId": "0b46e789-1f34-4733-e6e9-ee29c5036bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM :\n",
      "[[4815   10]\n",
      " [  91  656]]\n",
      "\n",
      "Naive Bayes :\n",
      "[[4823    2]\n",
      " [ 172  575]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv_pred_a = cross_val_predict(pipe_a, X, y, cv=10)\n",
    "cv_pred_b = cross_val_predict(pipe_b, X, y, cv=10)\n",
    "\n",
    "print('SVM :')\n",
    "print(confusion_matrix(y, cv_pred_a))\n",
    "print('\\nNaive Bayes :')\n",
    "print(confusion_matrix(y, cv_pred_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUhM58OsjQzz"
   },
   "source": [
    "En general, se obtienen mejores resultados con el modelo SVM, pero cabe destacar que utilizando Naive Bayes se producen menos errores tipo II, ya que asigna solo 2 ejemplos del tipo *ham* a la categoría *spam*.\n",
    "\n",
    "El SVM representa un modelo de mayor exactitud, pero siendo este relativamente más lento, y produciendo más errores de tipo II."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u3M9sf_YCmLW"
   },
   "source": [
    "# Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lomtLFXf2jlj"
   },
   "source": [
    "## Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLeVJpki2mlS"
   },
   "outputs": [],
   "source": [
    "#!pip install openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kK2aW4KiCppY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import openml\n",
    "\n",
    "dataset_har = openml.datasets.get_dataset(1478)\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset_har.get_data(\n",
    "    dataset_format='dataframe',\n",
    "    target=dataset_har.default_target_attribute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "J7kR8--l4Lfi",
    "outputId": "ba5be46c-40ee-4aca-908b-4960dff53903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 4, 6, 1, 3, 2]\n",
       "Categories (6, object): [1 < 2 < 3 < 4 < 5 < 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos información sobre la clase\n",
    "print(dataset_har.default_target_attribute)\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "SrxGcZxcYHn-",
    "outputId": "80c1fe7c-75b3-4122-ac73-3ff5864db76f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    1944\n",
       "5    1906\n",
       "4    1777\n",
       "1    1722\n",
       "2    1544\n",
       "3    1406\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cantidad de elementos en cada categoría\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tD2gS0s3Xi5M",
    "outputId": "1b3c61f9-578d-4fe3-bcda-62bef67365b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10299, 561)"
      ]
     },
     "execution_count": 118,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbPaa0P0V02x"
   },
   "source": [
    "Se puede ver que los atributos del dataset pueden corresponder a seis categorias diferentes, identificadas con números enteros del 1 al 6. El dataset consiste además de 10299 filas y 561 atributos.\n",
    "\n",
    "Según la información brindada en `openml`, sabemos que las mismas corresponden a las actividades que realizan usuarios, mientras se releva información del movimiento de los mismos. Estas actividades son: \n",
    "\n",
    "WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING\n",
    "\n",
    "Se asume que las mismas están identificadas numéricamente con el orden en que figuran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "1eXJlvS7v_AF",
    "outputId": "55dbfda3-b1d7-4de8-b1a3-5f047ee09d61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'WALKING',\n",
       " 2: 'WALKING_UPSTAIRS',\n",
       " 3: 'WALKING_DOWNSTAIRS',\n",
       " 4: 'SITTING',\n",
       " 5: 'STANDING',\n",
       " 6: 'LAYING'}"
      ]
     },
     "execution_count": 119,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_class = dict(enumerate(('WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', \n",
    "                             'SITTING', 'STANDING', 'LAYING'), start=1))\n",
    "dict_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-GZNU02DVjSe"
   },
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dWG5bBMXRRZ"
   },
   "source": [
    "Se observa información básica sobre el dataset, a modo de evaluar la realización de preprocesamiento sobre el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "pxNjlg6lVqzr",
    "outputId": "b3084dc0-bb91-41c1-ff08-22675a94a17d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>...</th>\n",
       "      <th>V522</th>\n",
       "      <th>V523</th>\n",
       "      <th>V524</th>\n",
       "      <th>V525</th>\n",
       "      <th>V526</th>\n",
       "      <th>V527</th>\n",
       "      <th>V528</th>\n",
       "      <th>V529</th>\n",
       "      <th>V530</th>\n",
       "      <th>V531</th>\n",
       "      <th>V532</th>\n",
       "      <th>V533</th>\n",
       "      <th>V534</th>\n",
       "      <th>V535</th>\n",
       "      <th>V536</th>\n",
       "      <th>V537</th>\n",
       "      <th>V538</th>\n",
       "      <th>V539</th>\n",
       "      <th>V540</th>\n",
       "      <th>V541</th>\n",
       "      <th>V542</th>\n",
       "      <th>V543</th>\n",
       "      <th>V544</th>\n",
       "      <th>V545</th>\n",
       "      <th>V546</th>\n",
       "      <th>V547</th>\n",
       "      <th>V548</th>\n",
       "      <th>V549</th>\n",
       "      <th>V550</th>\n",
       "      <th>V551</th>\n",
       "      <th>V552</th>\n",
       "      <th>V553</th>\n",
       "      <th>V554</th>\n",
       "      <th>V555</th>\n",
       "      <th>V556</th>\n",
       "      <th>V557</th>\n",
       "      <th>V558</th>\n",
       "      <th>V559</th>\n",
       "      <th>V560</th>\n",
       "      <th>V561</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>...</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "      <td>10299.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.89</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.72</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             V1        V2        V3  ...      V559      V560      V561\n",
       "count  10299.00  10299.00  10299.00  ...  10299.00  10299.00  10299.00\n",
       "mean       0.27     -0.02     -0.11  ...     -0.50      0.06     -0.05\n",
       "std        0.07      0.04      0.05  ...      0.51      0.31      0.27\n",
       "min       -1.00     -1.00     -1.00  ...     -1.00     -1.00     -1.00\n",
       "25%        0.26     -0.02     -0.12  ...     -0.82      0.00     -0.13\n",
       "50%        0.28     -0.02     -0.11  ...     -0.72      0.18     -0.00\n",
       "75%        0.29     -0.01     -0.10  ...     -0.52      0.25      0.10\n",
       "max        1.00      1.00      1.00  ...      1.00      1.00      1.00\n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "M6osIihI6Gd0",
    "outputId": "004d61a3-88b2-4dd0-8bc1-2a35aa4e7aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeMfnud2ZRX2"
   },
   "source": [
    "El dataset no contiene celdas con `NaN`, además los atributos están normalizados en el rango [-1, 1], por esto no se considera necesario aplicar tareas de preprocesamiento sobre el mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tq6W6i-FZhqa"
   },
   "source": [
    "## Generación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UH7wKoatgXpN"
   },
   "source": [
    "Se evita utilizar Naive Bayes, dado que se estima que por su naturaleza, los atributos están fuertemente correlacionados. Tampoco se desea aplicar k-NN, dado que sería muy costoso el entrenamiento por la cantidad de ejemplos y atributos.\n",
    "\n",
    "Se determina el uso de un árbol de decisión, que permite de requerirlo, una visualización de la toma de decisiones que se llevan a cabo. Por otro lado, se aplica el modelo de centroide más cercano, que ofrece una alternativa más rápida y que requiere menor almacenamiento respecto de k-NN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BS_PSds1ZjIP"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "clf_a = DecisionTreeClassifier(random_state=0)\n",
    "clf_b = NearestCentroid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-o-v4--xZjdT"
   },
   "source": [
    "## Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuMGYaKj6dsr"
   },
   "source": [
    "De manera similar que para el dataset anterior, se evaluan los modelos utilizando validación cruzada, para un `k=10`, dado que se tiene una buena cantidad de ejemplos.\n",
    "\n",
    "En este caso no se opta por utilizar la alternativa estratificada, dado que la cantidad de ejemplos por clase no tiene una gran variación. Se computa el Accuracy junto con el F-1 Score, en este caso mediante micro-averaging por la misma razón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rm-StVBuZlCy"
   },
   "outputs": [],
   "source": [
    "mscoring = ['f1_micro', 'accuracy']\n",
    "\n",
    "scores_a = cross_validate(clf_a, X, y, cv=10, scoring=scoring)\n",
    "scores_b = cross_validate(clf_b, X, y, cv=10, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "colab_type": "code",
    "id": "dUU1lAWxvxZ8",
    "outputId": "5ea4b77d-0369-4038-f966-36b0a6cd023e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 micro</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Fit time</th>\n",
       "      <th>Score time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Árbol de decisión</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Centroide más cercano</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       F1 micro  Accuracy  Fit time  Score time\n",
       "Árbol de decisión          0.87      0.87      8.01        0.01\n",
       "Centroide más cercano      0.82      0.82      0.07        0.02"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[scores_a['test_f1_micro'].mean(), scores_a['test_accuracy'].mean(), \n",
    "               scores_a['fit_time'].mean(), scores_a['score_time'].mean()],\n",
    "              [scores_b['test_f1_micro'].mean(), scores_b['test_accuracy'].mean(),\n",
    "               scores_b['fit_time'].mean(), scores_b['score_time'].mean()]]\n",
    "             , index=['Árbol de decisión', 'Centroide más cercano'], \n",
    "             columns=['F1 micro','Accuracy','Fit time','Score time']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHUg6NayT_fQ"
   },
   "source": [
    "El árbol de decisión muestra mejores resultados en ambas métricas. Su tiempo de predicción es menor que para el centroide más cercano, pero requiere una cantidad considerablemente mayor de tiempo de entrenamiento.\n",
    "\n",
    "A continuación, se realizan predicciones con `cross_val_predict` y se computa la matriz de confusión para cada modelo. En este caso, la misma se normaliza para una visualización más simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "lefkzP3Zv3-Y",
    "outputId": "d0658792-61c2-4354-cbf5-c8e630c41901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árbol de decisión :\n",
      "[[0.85 0.08 0.06 0.   0.   0.  ]\n",
      " [0.1  0.8  0.1  0.   0.   0.  ]\n",
      " [0.04 0.1  0.86 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.84 0.15 0.02]\n",
      " [0.   0.   0.   0.16 0.83 0.  ]\n",
      " [0.   0.   0.   0.   0.   1.  ]]\n",
      "\n",
      "Centroide más cercano :\n",
      "[[0.72 0.14 0.14 0.   0.   0.  ]\n",
      " [0.03 0.87 0.1  0.   0.   0.  ]\n",
      " [0.1  0.13 0.77 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.75 0.22 0.03]\n",
      " [0.   0.   0.   0.22 0.78 0.  ]\n",
      " [0.   0.01 0.   0.   0.   0.99]]\n"
     ]
    }
   ],
   "source": [
    "cv_pred_a = cross_val_predict(clf_a, X, y, cv=10)\n",
    "cv_pred_b = cross_val_predict(clf_b, X, y, cv=10)\n",
    "\n",
    "print('Árbol de decisión :')\n",
    "print(confusion_matrix(y, cv_pred_a, normalize='true').round(2))\n",
    "print('\\nCentroide más cercano :')\n",
    "print(confusion_matrix(y, cv_pred_b, normalize='true').round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Pv9vfI2iV6Ao",
    "outputId": "6b45cb0e-7283-472d-d1d1-a1d757e22dee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WALKING_UPSTAIRS'"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_class[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iSEtQtCUU6nM"
   },
   "source": [
    "La matriz de confusión también muestra una mayor exactitud utilizando árbol de decisión. Esto se cumple en todas las clases salvo para la segunda clase (*'WALKING_UPSTAIRS'*), donde se obtiene un mejor valor mediante centroide más cercano. Para el primer modelo, se ve que hay más casos donde se predice erronamente la primer clase.\n",
    "\n",
    "Para finalizar, se realiza un reporte de métricas diferenciadas por clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "colab_type": "code",
    "id": "QZXGW9Ad7aDD",
    "outputId": "b71263ff-03a9-468f-b855-2180c4011163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árbol de decisión :\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.87      0.85      0.86      1722\n",
      "  WALKING_UPSTAIRS       0.81      0.80      0.80      1544\n",
      "WALKING_DOWNSTAIRS       0.82      0.86      0.84      1406\n",
      "           SITTING       0.82      0.84      0.83      1777\n",
      "          STANDING       0.86      0.83      0.84      1906\n",
      "            LAYING       0.98      1.00      0.99      1944\n",
      "\n",
      "          accuracy                           0.87     10299\n",
      "         macro avg       0.86      0.86      0.86     10299\n",
      "      weighted avg       0.87      0.87      0.87     10299\n",
      "\n",
      "\n",
      "Centroide más cercano :\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.87      0.72      0.78      1722\n",
      "  WALKING_UPSTAIRS       0.75      0.87      0.80      1544\n",
      "WALKING_DOWNSTAIRS       0.74      0.77      0.75      1406\n",
      "           SITTING       0.76      0.75      0.76      1777\n",
      "          STANDING       0.79      0.78      0.79      1906\n",
      "            LAYING       0.98      0.99      0.98      1944\n",
      "\n",
      "          accuracy                           0.82     10299\n",
      "         macro avg       0.81      0.81      0.81     10299\n",
      "      weighted avg       0.82      0.82      0.82     10299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Árbol de decisión :')\n",
    "print(classification_report(y, cv_pred_a, target_names=dict_class.values()))\n",
    "print('\\nCentroide más cercano :')\n",
    "print(classification_report(y, cv_pred_b, target_names=dict_class.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WntRtH_qRpaA"
   },
   "source": [
    "La misma observación se realiza para el reporte, el árbol de decisión muestra para todos los casos mejores resultados, salvo para el Recall en la clase (*'WALKING_UPSTAIRS'*), o sea, la cantidad de ejemplos de esa clase que fueron correctamente clasificados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con scikit-learn\n",
    "\n",
    "En el presente trabajo se cargan dos datasets diferentes, uno conteniendo documentos de correos eléctronicos, y el otro con información relativa a los movimientos de usuarios de celulares, a partir de las mediciones realizadas con un acelerómetro. Ambos consisten en problemas de clasificación, teniendo en el primer caso un atributo objetivo binario (*spam y no spam*), y en el segundo caso uno multiclase (*las actividades que los usuarios realizan*).\n",
    "\n",
    "Se aplican y comparan para cada caso dos modelos de clasificación diferentes de la biblioteca `scikit-learn`, estos son:\n",
    "\n",
    "* `SVC`\n",
    "* `MultinomialNB`\n",
    "* `DecisionTreeClassifier`\n",
    "* ``Nearest Centroid`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
